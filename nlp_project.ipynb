{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../data/\" # On avait \"../../data/\" dans le fichier fourni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        similarWords = np.array([[key,self.score(w, key)]for key in self.word2vec.keys() if w!=key])\n",
    "        similarWords = similarWords[np.argsort(similarWords[:,1])]\n",
    "        similarWords = similarWords[-K:,0]\n",
    "        return(np.flipud(similarWords))\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        vec1 = self.word2vec[w1]\n",
    "        vec2 = self.word2vec[w2]\n",
    "        score = np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "        return(round(score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.6717\n",
      "dog pet 0.6842\n",
      "dogs cats 0.7074\n",
      "paris france 0.7775\n",
      "germany berlin 0.742\n",
      "['cats' 'kitty' 'kitten' 'feline' 'kitties']\n",
      "['dogs' 'puppy' 'Dog' 'doggie' 'canine']\n",
      "['dog' 'pooches' 'Dogs' 'doggies' 'canines']\n",
      "['france' 'Paris' 'london' 'berlin' 'tokyo']\n",
      "['austria' 'europe' 'german' 'berlin' 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False, sentence = False): #sentence est la phrase qu'on veut encoder c'est un string\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        sentemb = [] #array of sentences embeddings\n",
    "        listWords = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.split()\n",
    "            for word in sent:\n",
    "                listWords.append(word)\n",
    "        listWords = list(set(listWords)) #List which contains all words of the document\n",
    "        #We enter in this if only if we want to encode only one string\n",
    "        if sentence!= False:\n",
    "            sent = sentence.split()\n",
    "            l = np.zeros(len(listWords)); #list which contain occurence of each word in a sentence\n",
    "            for word in enumerate(listWords):\n",
    "                if idf is False:\n",
    "                    # mean of word vectors\n",
    "                    for wordSentence in sent:\n",
    "                        if wordSentence == word[1]:\n",
    "                            l[word[0]] +=1\n",
    "                else:\n",
    "                    # idf-weighted mean of word vectors\n",
    "                    for wordSentence in sent:\n",
    "                        if wordSentence == word[1]:\n",
    "                            l[word[0]] +=idf[wordSentence]\n",
    "            sentemb.append(l)\n",
    "            return np.vstack(sentemb)\n",
    "        #We enter in this boucle when we want to encode all the file\n",
    "        for sent in sentences[:1000]: #We are interested in the first 100 sentences to limit the calcul\n",
    "            sent = sent.split()\n",
    "            l = np.zeros(len(listWords)); #list which contain occurence of each word in a sentence\n",
    "            for word in enumerate(listWords):\n",
    "                if idf is False:\n",
    "                    # mean of word vectors\n",
    "                    for wordSentence in sent:\n",
    "                        if wordSentence == word[1]:\n",
    "                            l[word[0]] +=1\n",
    "                else:\n",
    "                    # idf-weighted mean of word vectors\n",
    "                    for wordSentence in sent:\n",
    "                        if wordSentence == word[1]:\n",
    "                            l[word[0]] +=idf[wordSentence]\n",
    "            sentemb.append(l)\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        sentencesVectors = self.encode(sentences, idf)\n",
    "        query = self.encode(sentences, idf, s)[0]\n",
    "        similarSentences = np.array([[indice,self.score(query, sentence)]for indice,sentence in enumerate(sentencesVectors)])\n",
    "        similarSentences = similarSentences[np.argsort(similarSentences[:,1])]\n",
    "        similarSentences = similarSentences[-K-1:-1,:]\n",
    "        similarSentences = np.flipud(similarSentences)\n",
    "        L = []\n",
    "        print(\"The \", str(K), \" sentences the most similar to '\", s, \"' are:\")\n",
    "        for i in similarSentences:  \n",
    "            print(sentences[int(i[0])])\n",
    "            L.append(sentences[int(i[0])])\n",
    "        return(L)\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        score = np.dot(s1,s2)/(np.linalg.norm(s1)*np.linalg.norm(s2))\n",
    "        return(round(score,4))\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in list(set(sent.split())):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        for word in idf.keys():\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "        return(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "The  5  sentences the most similar to ' 1 smiling african american boy .  ' are:\n",
      "5 women and 1 man are smiling for the camera . \n",
      "a african teacher , smiling , while working with baby on back . \n",
      "2 kids holding hands and smiling . \n",
      "2 african adults and 8 african children looking at pictures on a table . \n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "0.0765\n",
      "The  5  sentences the most similar to ' 1 smiling african american boy .  ' are:\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "5 women and 1 man are smiling for the camera . \n",
      "2 guys facing away from camera , 1 girl smiling at camera with blue shirt , 1 guy with a beverage with a jacket on . \n",
      "1 woman in a black jacket is drinking out of a bottle while others are smiling . \n",
      "1 man riding a bike through the country . \n",
      "0.0134\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'sentences.txt'), 'r') as file:\n",
    "    txt = file.read()\n",
    "sentences = txt.split(\"\\n\")\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "print(s2v.score('' if not sentences else s2v.encode(sentences, False, sentences[7])[0], '' if not sentences else s2v.encode(sentences, False, sentences[13])[0]))\n",
    "\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print(s2v.score('' if not sentences else s2v.encode(sentences, idf, sentences[7])[0], '' if not sentences else s2v.encode(sentences, idf, sentences[13])[0], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 pretrained word vectors\n",
      "Loaded 10000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "frDictionnary = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=10000)\n",
    "enDictionnary = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "commonWords = []\n",
    "for i in frDictionnary.word2vec.keys():\n",
    "    if i in enDictionnary.word2vec:\n",
    "        commonWords.append(i)\n",
    "\n",
    "X = [] \n",
    "Y = []\n",
    "\n",
    "for word in commonWords:\n",
    "    X.append(frDictionnary.word2vec[word])\n",
    "    Y.append(enDictionnary.word2vec[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import scipy.linalg\n",
    "M = np.dot(Y,np.transpose(X))\n",
    "U, Sig, V = scipy.linalg.svd(M, full_matrices = True)\n",
    "W = np.dot(U,np.transpose(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five words the more similar to paris in english are:\n",
      "moral\n",
      "questions\n",
      "answers\n",
      "thoughts\n",
      "judgement\n",
      "The five words the more similar to jean in english are:\n",
      "difficulty\n",
      "scenes\n",
      "legendary\n",
      "norse\n",
      "hank\n",
      "The five words the more similar to cat in english are:\n",
      "gmina\n",
      "delaware\n",
      "paragraphs\n",
      "footnote\n",
      "prose\n",
      "The five words the more similar to paris in french are:\n",
      "régent\n",
      "mariée\n",
      "héritier\n",
      "détenu\n",
      "décoré\n",
      "The five words the more similar to jean in french are:\n",
      "par\n",
      "composé\n",
      "associés\n",
      "trad\n",
      "vendu\n",
      "The five words the more similar to cat in french are:\n",
      "famille\n",
      "enfants\n",
      "johan\n",
      "figurer\n",
      "classer\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "Xpredict = np.dot(np.transpose(W),Y)\n",
    "Ypredict = np.dot(W,X)\n",
    "\n",
    "def score(vec1, vec2):\n",
    "        score = np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "        return(round(score,4))\n",
    "\n",
    "# paris indice 52, jean indice 55, cat indice 1819 dans la liste commonWords\n",
    "\n",
    "L = {'paris' : 52, 'jean' : 55, 'cat' : 1819}\n",
    "\n",
    "#Français -> Anglais\n",
    "\n",
    "for key, value in L.items():\n",
    "    similarWords = np.array([[word,score(vector, Ypredict[value])]for word, vector in enDictionnary.word2vec.items()])\n",
    "    similarWords = similarWords[np.argsort(similarWords[:,1])]\n",
    "    similarWords = np.flipud(similarWords[-5:,0])\n",
    "    print(\"The five words the more similar to \" + str(key) + \" in english are:\")\n",
    "    for word in similarWords:\n",
    "        print(word)\n",
    "\n",
    "#Anglais -> Français\n",
    "\n",
    "for key, value in L.items():\n",
    "    similarWords = np.array([[word,score(vector, Xpredict[value])]for word, vector in frDictionnary.word2vec.items()])\n",
    "    similarWords = similarWords[np.argsort(similarWords[:,1])]\n",
    "    similarWords = np.flipud(similarWords[-5:,0])\n",
    "    print(\"The five words the more similar to \" + str(key) + \" in french are:\")\n",
    "    for word in similarWords:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "sentencesTrain = []\n",
    "sentencesDev = []\n",
    "sentencesTest = []\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'r') as file:\n",
    "    txtTrain = file.read()\n",
    "sentencesTrain = txtTrain.split(\"\\n\")\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'r') as file:\n",
    "    txtDev = file.read()\n",
    "sentencesDev = txtDev.split(\"\\n\")\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'r') as file:\n",
    "    txtTest = file.read()\n",
    "sentencesTest = txtTest.split(\"\\n\")\n",
    "\n",
    "sentencesTrain = [[sentence[1:] , sentence[0]] for sentence in sentencesTrain if len(sentence) >= 1]\n",
    "sentencesDev = [[sentence[1:] , sentence[0]] for sentence in sentencesDev if len(sentence) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "sentences = np.concatenate((np.array(sentencesTrain)[:100,0], np.array(sentencesDev)[:100,0], np.array(sentencesTest)[:100]), axis = 0)\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=50000)\n",
    "s2v = BoV(w2v)\n",
    "idf = s2v.build_idf(sentences)\n",
    "encodeTestSet = [s2v.encode(sentences,idf,sentence)[0] for sentence in sentencesTest]\n",
    "encodeTrainSet = [s2v.encode(sentences,idf,sentence)[0] for sentence in np.array(sentencesTrain)[:,0]]\n",
    "encodeDevSet = [s2v.encode(sentences,idf,sentence)[0] for sentence in np.array(sentencesDev)[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.6178604868913857Dev accuracy 0.3678474114441417\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "#Comparer avec et sans idf\n",
    "#Essayer avec tous le dataset\n",
    "#Utiliser le pca\n",
    "#Enlever les mots de deux lettre ou moins, Sert à rien\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = encodeTrainSet\n",
    "y = np.array(sentencesTrain)[:,1]\n",
    "clf = LogisticRegression(C = 0.25, penalty = 'l2', solver='newton-cg', multi_class='multinomial', random_state = 0).fit(X, y)\n",
    "print(\"Train accuracy \" + str(clf.score(X, y)) + \"Dev accuracy \" + str(clf.score(encodeDevSet, np.array(sentencesDev)[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "yTest = clf.predict(encodeTestSet)\n",
    "fichier = open(\"logreg_bov_y_test_sst.txt\", \"a\")\n",
    "fichier.write(yTest[0])\n",
    "for i in yTest[1:]:\n",
    "    fichier.write(\"\\n\" + i)\n",
    "fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.6405664794007491Dev accuracy 0.3460490463215259\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn import svm\n",
    "bdt = svm.LinearSVC(multi_class='ovr', penalty='l2')\n",
    "bdt.fit(X,y)\n",
    "print(\"Train accuracy \" + str(bdt.score(X, y)) + \"Dev accuracy \" + str(bdt.score(encodeDevSet, np.array(sentencesDev)[:,1])))\n",
    "\n",
    "yTestSVM = bdt.predict(encodeTestSet)\n",
    "fichier = open(\"SVM_bov_y_test_sst.txt\", \"a\")\n",
    "fichier.write(yTestSVM[0])\n",
    "for i in yTestSVM[1:]:\n",
    "    fichier.write(\"\\n\" + i)\n",
    "fichier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 0  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1de096f8c6e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m model.compile(loss=loss_classif,\n\u001b[0;32m     11\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m               metrics=metrics_classif)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    766\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                                     printable_module_name='optimizer')\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 138\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown optimizer: "
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  '' # find the right loss for multi-class classification\n",
    "optimizer        =  '' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
